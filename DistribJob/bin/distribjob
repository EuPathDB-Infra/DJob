#!@perl@

use lib "$ENV{GUS_HOME}/lib/perl";

use DJob::DistribJob::Controller;
use Getopt::Long;

my($parInit,$runTime,$propFile,$help,$kill,$killslow,$numNodes,$fileName,$hostname, $queue);
my $procsPerNode = 2;
my $memPerNode = 1.8;

&GetOptions("help|h!" => \$help, 
            "kill|k!"=> \$kill,
            "killslow|ks!" => \$killslow,
            "numNodes|n=i" => \$numNodes,
            "runTime|t=i" => \$runTime,
            "procsPerNode|ppn=i" => \$procsPerNode,
            "memoryPerNode|mpn=s" => \$memPerNode,
            "parallelInit|pi=i" => \$parInit,
            "propFile|p=s" => \$propFile,
            "fileName|f=s" => \$fileName,
            "hostname|h=s" => \$hostname,
            "queue|q=s" => \$queue,
            );

help() if $help;

unless (-e "$propFile"){
    usage(); 
    exit(0);
}


my $killVal = $kill ? 1 : $killslow ? 2 : 0;

DJob::DistribJob::Controller->new($propFile, $numNodes ? $numNodes : \@ARGV ,$killVal, $runTime, $parInit, $fileName, $hostname, $procsPerNode, $memPerNode, $queue);

sub usage {

    print "
Usage:
  distribjob --help
  distribjob --propFile propfile --numNodes <number of nodes> --runTime <minutes for expected run> --parallelInit <number of nodes to initialize task on at a time> --procsPerNode <number of processers per node [2]> --memoryPerNode|mpn <GB of memory per node [1.8]> --queue <queue name>
  distribjob --propFile propfile node1 node2...
  distribjob --propFile propfile -kill
  distribjob --propFile propfile -killslow

where:

  --help|h            Print full help.

  --propFile|p        A file containing configuration properties for the distributed
                      job controller.  (See The Controller Properties File in full 
                      help).

  --numNodes|n        Number of nodes to run on ... only valid if there is no list of nodes

  --parallelInit|pi   Number of ndoes to initialize the task on at a time

  --procsPerNode|ppn  Number of processors to request / node [default is  2]

  --memoryPerNode|mpn  gigabytes of memory to request / node [default is  1.8]

  --queue|q           Name  of queue to use if not using the default queue

  --fileName|f        file name to be passed to the controller that nodes can utilize

  --kill|k            Gracefully kill the controller and node subtasks as soon as 
                      possible.
  
  --killslow|ks       Gracefully kill the controller when all current node subtasks
                      are done.

  node1 ...	      List of nodes to run on.
";
}

sub help {

    &usage();

    print "
Distributed Job Controller
  distribjob is a general purpose distributed job controller with a simple
  interface so you can use easily run and monitor a distributed job.  It
  also has a simple API so you can extend it to distribute your type of job.

The Controller
  Your distributed job is managed by a controller program.  It divides your
  task into subtasks, and distributes those subtasks to nodes as they become
  available.  

  The controller makes the following assumptions:
    - your task has a list of inputs (eg, a list of sequences to operate on).

    - elements of the input list can be randomly accessed by their index.

    - the elements may be processed in any order.

    - the input can be divided into sublists of a fixed size (which you 
      determine) that will be distributed as subtasks to the nodes.

    - the results of subtasks can be merged in any order to form a correct 
      main result.

    - nodes need only be initialized once before the subtasks are distributed.

    - subtasks running on the node will run in a specified local directory that
      the controller creates before each subtask runs.  (Ie, subtasks
      cannot pass state to other subtasks through the node's local file
      system).

    - if a task is restarted, the input list indexing is the same as when
      the task ran previously.  (Ie, you may change but not add or remove 
      elements of the input list)


The Controller Properties File

   The controller uses a properties file.  Here is a sample:

masterdir=/home/you/yourtask/master
inputdir=/home/you/yourtask/input
nodedir=/scratch/you/yourtask
slotspernode=2
subtasksize=100
taskclass=YourSubclassOfTask
nodeclass=DJob::DistribJob::BprocNode
restart=no

  You may comment out lines using a #.  Here is an explanation of the 
  properties:

  masterdir     The controller's master directory (see below). Must 
                not exist, unless restarting, in which case it must 
                exist.  You must specify a full path.

  inputdir      Directory in which the controller can find the input. This 
                must include a file called task.prop which holds the properties
                your task needs.  It will also typically contain input data 
                files for the task.  This is also a convenient place to store 
                the controller's property file.  You must specify a full path.
 
  nodedir       Directory on the node's local disk to use as a working dir.  
                You must specify a full path.  (When simulating nodes by using 
                LocalNode as the nodeClass, this is the directory where the 
                simulated nodes' 'local disks' are placed.)

  slotspernode  Number of subtasks that run on each node simultaneously.  For
                example, if each node has two processors, and the command you
                are running does not automatically parallelize you might run
                two or more slots per node to try to use the CPUs optimally.

  subtasksize   Number of elements of the input list to include in each subtask
                distributed to a node at one time (minimum is 1).  You should
                tune this number to compensate for the overhead of copying
                input files to and results files from the node.  Typically,
                you would like your subtasks to run for at least a few seconds
                and not more than a bunch of minutes, if possible.  The fewer
                elements you include in your subtasks, the finer the 
                granularity of failure (which is good), but if you have too
                few, you may incur unnecessary overhead costs.

  taskclass	Subclass of DJob::DistribJob::Task that manages your task

  nodeclass     Subclass of DJob::DistribJob::Node that handles the node

  restart       yes/no.  Use 'no' the first time your task runs on your input.
                (The controller will create the master directory).  Use 'yes' 
                if you have killed your run or it completed with failures and 
                you want to restart it (be sure to correct the causes of any
                failures found in the master/failures directory, and then 
                delete master/failures).


Your Subclass of DJob::DistribJob::Task.

  DJob::DistribJob::Task is an object which manages your task.  You must provide
  a subclass of it to do certain operations specific to your task:
    - initializing a node
    - providing the size of your input list
    - initializing a subtask-specific directory with the subtask's input
    - running the subtask
    - merging the result

  See lib/perl/DistribJob/TestTask.pm for a detailed template of 
  this subclass. 


Your Subclass of DJob::DistribJob::Node

  The release includes two subclasses of node: DJob::DistribJob::BprocNode (for
  beowulf/bproc based clusters) and DJob::DistribJob::LocalNode (to simulate nodes
  on a single local machine).  See those subclasses if you want to write 
  another.


The Master Directory
  The Master Directory is a working space for the controller, and is where
  it places your results, and information about running and failed subtasks.
  This is how it is structured:
   
     your_masterdir/
        completedSubtasks.log  # list of completed subtasks (starting w/ 1)
        failed/             # contains a dir for each failed subtask
          23/
          89/
          ...
        mainResult/
          your_answer       # merged by your Task subclass from subtask results
        running/            # contains a dir for each running subtask
          100/
          101/
          ...

   The completed.log file is created by the controller.  It contains a list of 
   successfully completed subtasks, so that they won't be re-run on restart.
   If you need to re-run a particular subtask, remove it from this file
   before restarting.

   The failed/ directory contains a subdirectory for each subtask that has 
   failed.  It will include all log, result and temporary files created by 
   the subtask when running on a node.  You must delete the subtask's
   directory from failed/ before restarting otherwise it will not run.

   The mainResult/ directory holds the main result that your Task subclass 
   constructs by merging individual results from the subtasks.

   The running/ directory contains a subdirectory for each running subtask.
   Each subdirectory contains the specific input for that subtask.  It will 
   also receive the results from the node when the subtask is done.  (Your 
   task subclass merge those results into your main result.)  The subtask's 
   directory is deleted when it has succeeded (after merging its result)
   or failed.  

";
    exit(0);
}

1;


        
      
